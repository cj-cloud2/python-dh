## Lab 3: The Architect (Pandas Data Manipulation)

In the previous labs, you handled raw strings and numerical arrays. Now, you will step into the role of a Data Architect using **Pandas**. Pandas is built on top of NumPy but adds structure (like an Excel sheet) called a **DataFrame**.

In this lab, you will repair broken data, convert text dates into real time objects, and merge two separate datasetsâ€”a critical skill for any Data Engineer.

### Detailed Requirements for Lab 3

1. **DataFrame Creation:** Load the raw CSV data into a Pandas DataFrame.
2. **String Normalization:** Fix the inconsistent capitalization in the `transit_type` column (e.g., turning "bus", "Bus", "BUS" into "Bus") using Pandas string methods.
3. **Temporal Engineering:** Convert the `timestamp` column from a generic "Object" (string) type to a `datetime64` object to extract specific hours.
4. **Relational Merging:** Create a secondary dataset of **Station Names** and join it with your ridership data using `station_id` as the key (Left Join).
5. **Aggregation:** Create a Pivot Table that shows the total revenue generated by each Transit Type.

---

### Step 1: Setup & Data Generation

We need to ensure our main data exists, and we need to create a *new* second file (`station_meta.csv`) to demonstrate how to merge two different sources.

**Step Description:**
Create the main ridership CSV (if not already present) and a new Station Metadata CSV containing human-readable station names.

**Logic & Planning:**

```python
# Import pandas library.
# Define string data for the main transit csv.
# Write the main transit data to 'transit_data.csv'.
# Define string data for the station metadata csv (Station ID and Name).
# Write the metadata to 'station_meta.csv'.

```

**Solution:**

```python
# Import the pandas library with the standard alias 'pd'
import pandas as pd

# Define the raw CSV content again to ensure the file exists in this session
csv_data = """timestamp,station_id,transit_type,passenger_count,revenue,weather_temp
2025-01-01 08:00:00,STN_001,Bus,45,112.50,2.5
2025-01-01 08:15:00,STN_002,Metro,120,360.00,2.4
01/01/2025 08:30,STN_001,bus,38,,2.1
2025-01-01 09:00:00,STN_003,Metro,310,930.00,
2025-01-01 09:15:00,STN_002,Metro,,450.00,3.0
01-01-2025 09:30,STN_001,Bus,55,137.50,3.5
2025-01-01 10:00:00,STN_004,LightRail,22,66.00,4.2
2025-01-01 10:15:00,STN_002,METRO,150,450.00,4.5
2025-01-01 10:30:00,STN_001,Bus,NaN,100.00,4.8
2025-01-01 11:00:00,STN_003,Metro,280,840.00,5.0"""

# Write the main data to disk
with open('transit_data.csv', 'w') as f:
    f.write(csv_data)

# Define the secondary dataset: Station Metadata
meta_data = """station_id,station_name,location_zone
STN_001,Central Plaza,Downtown
STN_002,Airport Terminal,North
STN_003,Tech Park,East
STN_004,Suburban Hub,West"""

# Write the metadata to a new file named 'station_meta.csv'
with open('station_meta.csv', 'w') as f:
    f.write(meta_data)
    
print("Setup Complete: 'transit_data.csv' and 'station_meta.csv' created.")

```

---

### Step 2: Loading and Inspecting

Now we load the data into a DataFrame. This is your primary workspace in Pandas.

**Step Description:**
Read the CSV file into a DataFrame and display the first few rows to inspect the structure.

**Logic & Planning:**

```python
# Read 'transit_data.csv' into a DataFrame variable named df.
# Display the first 5 rows of the dataframe to check the data loading.
# Print the concise summary of the dataframe to check data types and nulls.

```

**Solution:**

```python
# Use pd.read_csv to load the data into a DataFrame called 'df'
df = pd.read_csv('transit_data.csv')

# Use the .head() method to print the first 5 rows for visual inspection
print(df.head())

# Use the .info() method to see column types and count non-null values
df.info()

```

---

### Step 3: Cleaning Text Data

In the output above, you'll see "Bus", "bus", and "METRO". We need to standardize this column so we can group them accurately later.

**Step Description:**
Access the `transit_type` column as a String Series and convert all values to "Title Case" (e.g., "Bus").

**Logic & Planning:**

```python
# Access the 'transit_type' column using the .str accessor.
# Apply the title() string method to normalize the text casing.
# Assign the cleaned values back to the 'transit_type' column.
# Print the unique values in this column to verify the fix.

```

**Solution:**

```python
# Use the .str.title() method to convert all strings in the column to Title Case
df['transit_type'] = df['transit_type'].str.title()

# Use .unique() to display all unique categories in the column to confirm cleanup
print(f"Categories: {df['transit_type'].unique()}")

```

---

### Step 4: Temporal Engineering (Dates)

Currently, the `timestamp` column is an "object" (text). We cannot extract the hour or sort by date efficiently. We must convert it to a real datetime object.

**Step Description:**
Convert the `timestamp` column to datetime objects using Pandas' flexible parser, which handles mixed formats (like `/` vs `-`) automatically.

**Logic & Planning:**

```python
# Convert the 'timestamp' column to datetime objects using to_datetime.
# Extract the 'hour' from the timestamp and create a new column called 'Hour'.
# Print the first few rows to see the new 'Hour' column.

```

**Solution:**

```python
# Use pd.to_datetime to convert the column, setting 'mixed' format inference
df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed')

# Use the .dt accessor to extract the hour integer from the datetime object
df['Hour'] = df['timestamp'].dt.hour

# Print the head again to verify the new 'Hour' column exists
print(df[['timestamp', 'Hour']].head())

```

---

### Step 5: Handling Missing Values

We have `NaN` in `passenger_count`. In this scenario, we will fill missing passenger counts with the median value of the column to avoid skewing our data with zeros.

**Step Description:**
Calculate the median of `passenger_count` and fill the `NaN` slots with that value.

**Logic & Planning:**

```python
# Calculate the median value of the 'passenger_count' column.
# Fill the NaN values in 'passenger_count' with the calculated median.
# Verify there are no more nulls in that column.

```

**Solution:**

```python
# Use .median() to find the middle value of the passenger_count data
median_riders = df['passenger_count'].median()

# Use .fillna() to replace NaNs with the median (inplace=True modifies original df)
df['passenger_count'].fillna(median_riders, inplace=True)

# Check the sum of null values in the column to confirm it is now 0
print(f"Nulls remaining: {df['passenger_count'].isnull().sum()}")

```

---

### Step 6: Merging DataFrames

We have station IDs (STN_001), but business executives want station names. We will join our dataframe with `station_meta.csv`.

**Step Description:**
Load the metadata CSV and merge it with our main DataFrame using a "Left Join" on the `station_id` column.

**Logic & Planning:**

```python
# Read the 'station_meta.csv' file into a new DataFrame.
# Merge the main df with the meta_df on 'station_id'.
# Use a 'left' join to ensure we keep all ridership data even if metadata is missing.
# Display the first few rows of the merged dataframe.

```

**Solution:**

```python
# Load the station metadata file into a dataframe named 'df_meta'
df_meta = pd.read_csv('station_meta.csv')

# Use pd.merge to join 'df' and 'df_meta' on the key 'station_id'
df_merged = pd.merge(df, df_meta, on='station_id', how='left')

# Print the columns to confirm 'station_name' and 'location_zone' were added
print(df_merged.columns)
print(df_merged[['station_id', 'station_name']].head())

```

---

### Step 7: Aggregation (The Analysis)

Finally, we want to know which mode of transport brings in the most revenue.

**Step Description:**
Group the data by `transit_type` and sum the `revenue` column.

**Logic & Planning:**

```python
# Group the merged dataframe by 'transit_type'.
# Select the 'revenue' column and calculate the sum.
# Print the resulting Series.

```

**Solution:**

```python
# Use .groupby() to cluster data by transit type, then sum the revenue
revenue_analysis = df_merged.groupby('transit_type')['revenue'].sum()

# Print the aggregated revenue data
print(revenue_analysis)

```

---

### Final Instructions: Run the Application

Combine all steps into a single block to witness the power of Pandas.

```python
import pandas as pd

# 1. Load Data
df = pd.read_csv('transit_data.csv')
df_meta = pd.read_csv('station_meta.csv')

# 2. Clean Text
df['transit_type'] = df['transit_type'].str.title()

# 3. Handle Dates
df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed')
df['Hour'] = df['timestamp'].dt.hour

# 4. Handle Missing Data (Imputation)
df['passenger_count'].fillna(df['passenger_count'].median(), inplace=True)

# 5. Merge with Metadata
df_final = pd.merge(df, df_meta, on='station_id', how='left')

# 6. Final Analysis
print("--- Total Revenue by Transit Type ---")
print(df_final.groupby('transit_type')['revenue'].sum())

print("\n--- Sample of Final Data ---")
print(df_final[['timestamp', 'station_name', 'transit_type', 'revenue']].head())

```
